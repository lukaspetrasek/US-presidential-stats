{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# US presidential statistics\n",
    "## Simon Repko, Lukas Petrasek\n",
    "### IES FSS CU\n",
    "### 31.5.2019\n",
    "\n",
    "This notebook serves as a demonstration of a school project whose goal is to achieve the following:\n",
    "* scrape web pages to get historical data on US presidents\n",
    "* manipulate the data into a form suitable for being visualized\n",
    "* make vizualizations based the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, Optional, Type\n",
    "import itertools\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly\n",
    "import requests\n",
    "\n",
    "\n",
    "\n",
    "plotly.offline.init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: initialize the scraping class and apply the methods necessary to get the data here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_soup(url: str) -> BeautifulSoup:\n",
    "    ''' Returns soup for the given url. '''\n",
    "    return BeautifulSoup(requests.get(url).text, 'html.parser')\n",
    "\n",
    "\n",
    "\n",
    "class MillerScraper:\n",
    "    '''\n",
    "    Class for scraping the Miller Center webpage to get data about US presidents. \n",
    "\n",
    "    Particularly, it serves for scraping data on facts and characteristics (self.fast_facts), \n",
    "    brief description (self.descriptions), famous quotes (self.famous_quotes), and count of \n",
    "    notable events happened during the president's office (self.key_events_counts).\n",
    "    '''\n",
    "\n",
    "    def __init__(self):\n",
    "        ''' Initializes the scraping class. '''\n",
    "        self.origin: str = 'https://millercenter.org/'\n",
    "\n",
    "        self.subdirectories: Optional[Dict[str, Any]] = None\n",
    "\n",
    "        self.fast_facts: Optional[Dict[str, Any]] = None\n",
    "        self.descriptions: Optional[Dict[str, Any]] = None\n",
    "        self.famous_quotes: Optional[Dict[str, Any]] = None\n",
    "        self.key_events_counts: Optional[Dict[str, Any]] = None\n",
    "\n",
    "        self.all_presidents_data: Optional[pd.DataFrame()] = None\n",
    "\n",
    "\n",
    "    def get_subdirectories(self) -> None:\n",
    "        '''\n",
    "        Creates a dictionary with the presidents' names as keys and their respective \n",
    "        subdirectories as values, then saves the dictionary to self.subdirectories.\n",
    "        '''\n",
    "        # parse the given origin (web address) utilizing BeautifulSoup \n",
    "        soup = get_soup(self.origin) \n",
    "        # enter the main navigation panel and find submenu that contains the links \n",
    "        # (subdirectories) to individual pages of US presidents\n",
    "        navigation_menu = soup.find('nav', {'aria-labelledby':'block-mainnavigation-3-menu'})\n",
    "        submenu = navigation_menu.find_all('ul', {'class':'submenu'})[1]\n",
    "        a_blocks = submenu.find_all('a')\n",
    "\n",
    "        subdirectories = {}\n",
    "        for a_block in tqdm(a_blocks): \n",
    "            # each a_block represents one president, extract the subdirectory and save \n",
    "            # it under the president's name\n",
    "            subdirectories[a_block.text] = a_block['href']\n",
    "\n",
    "        self.subdirectories = subdirectories\n",
    "\n",
    "\n",
    "    def get_fast_facts(self) -> None:\n",
    "        '''\n",
    "        Iterates over the subdirectories to get on the individual page of the respective \n",
    "        president and save the relevant fast facts into self.fast_facts. \n",
    "        '''\n",
    "        fast_facts = {}\n",
    "        for president, subdirectory in tqdm(self.subdirectories.items()):\n",
    "            # parse the given path (web address) utilizing BeautifulSoup\n",
    "            soup = get_soup(self.origin + subdirectory)\n",
    "            # navigate through the soup to get to the part relevant for fast facts\n",
    "            president_page = soup.find('div', {'class':'president-main-wrapper'})\n",
    "            fast_facts_dashboard = president_page.find('div', {'class':'fast-facts-wrapper'})   \n",
    "\n",
    "            # avoiding redundant elements (containing '\\n')\n",
    "            relevant_fast_facts = [x for x in fast_facts_dashboard.children if x != '\\n']\n",
    "            # popping the first element which contains just the 'Fast Facts' heading\n",
    "            relevant_fast_facts.pop(0)\n",
    "\n",
    "            fast_facts[president] = {}\n",
    "            for fast_fact in relevant_fast_facts:\n",
    "                # save the fast fact under its label into the dict of the given president\n",
    "                fast_facts[president][fast_fact.label.text] = fast_fact.div.text\n",
    "\n",
    "        self.fast_facts = fast_facts\n",
    "\n",
    "\n",
    "    def get_descriptions(self) -> None:\n",
    "        '''\n",
    "        Iterates over the subdirectories to get on the individual page of the respective \n",
    "        president and save the relevant description into self.descriptions. \n",
    "        '''\n",
    "        descriptions = {}\n",
    "        for president, subdirectory in tqdm(self.subdirectories.items()):\n",
    "            # parse the given path (web address) utilizing BeautifulSoup\n",
    "            soup = get_soup(self.origin + subdirectory)\n",
    "            # navigate through the soup to get to the part relevant for the description\n",
    "            description_paragraph = soup.find('div', {'class':'copy-wrapper'})\n",
    "\n",
    "            # save the description into the dict with descriptions\n",
    "            descriptions[president] = description_paragraph.p.text\n",
    "\n",
    "        self.descriptions = descriptions\n",
    "\n",
    "\n",
    "    def get_famous_quotes(self) -> None:\n",
    "        '''\n",
    "        Iterates over the subdirectories to get on the individual page of the respective \n",
    "        president and save the relevant famous quote into self.famous_quotes. \n",
    "        '''\n",
    "        famous_quotes = {}\n",
    "        for president, subdirectory in tqdm(self.subdirectories.items()):\n",
    "            # parse the given path (web address) utilizing BeautifulSoup\n",
    "            soup = get_soup(self.origin + subdirectory)\n",
    "            # navigate through the soup to get to the part relevant for the famous quote\n",
    "            famous_quote_paragraph = soup.find('blockquote', {'class':'president-quote'})\n",
    "\n",
    "            # save the famous quote into the dict with famous quotes\n",
    "            famous_quotes[president] = str(famous_quote_paragraph.contents[0])\n",
    "\n",
    "        self.famous_quotes = famous_quotes\n",
    "\n",
    "\n",
    "    def get_key_events_counts(self) -> None:\n",
    "        '''\n",
    "        Iterates over the subdirectories to get on the individual page of the respective \n",
    "        president and save the relevant key events count into self.key_events_counts. \n",
    "        '''\n",
    "        key_events_counts = {}\n",
    "        for president, subdirectory in tqdm(self.subdirectories.items()):\n",
    "            # parse the given path (web address) utilizing BeautifulSoup\n",
    "            soup = get_soup(self.origin + subdirectory + '/key-events')\n",
    "            # navigate through the soup to get to the part relevant for key events\n",
    "            key_events_overview = soup.find('div', {'class':'article-wysiwyg-body'})\n",
    "\n",
    "            try:\n",
    "                # count of all events ('titles' highlighted in bold)\n",
    "                key_events_count_bold = len(key_events_overview.find_all('strong'))\n",
    "                # count of all events ('titles' no longer highlighted in bold)\n",
    "                key_events_count_not_bold = len(key_events_overview.find_all('b'))\n",
    "                # sum both counts\n",
    "                key_events_count = key_events_count_bold + key_events_count_not_bold\n",
    "            # D. Trump page has no information about major events, hence the exception \n",
    "            except AttributeError:\n",
    "                key_events_count = 0\n",
    "                pass\n",
    "\n",
    "            # save the key events count into the dict with key events counts\n",
    "            key_events_counts[president] = key_events_count\n",
    "\n",
    "        self.key_events_counts = key_events_counts\n",
    "\n",
    "\n",
    "    def correct_Grover_Cleveland_data(self) -> None:\n",
    "        '''\n",
    "        Corrects Grover Cleveland's data. \n",
    "        \n",
    "        Because, due to Grover Cleveland being in office 2 non-consecutive times, the \n",
    "        'Inauguration Date', 'Date Ended' and 'President Number' facts are present twice \n",
    "        in the data.\n",
    "\n",
    "        Also, duplicates other Grover Cleveland's data so that it is entered for each of\n",
    "        his terms.\n",
    "        '''\n",
    "        # assert that 'Grover Cleveland 2' entry doesn't exist already\n",
    "        assert 'Grover Cleveland 2' not in miller_scrape.fast_facts.keys()\n",
    "\n",
    "        # create entries for Cleveland's second term\n",
    "        self.fast_facts['Grover Cleveland 2'] = {\n",
    "            key: value for key, value in self.fast_facts['Grover Cleveland'].items()\n",
    "        }\n",
    "        for attribute in [self.descriptions, self.famous_quotes, self.key_events_counts]:\n",
    "            attribute['Grover Cleveland 2'] = attribute['Grover Cleveland']\n",
    "\n",
    "        # input corrected entries\n",
    "        for entry_name in ['Inauguration Date', 'Date Ended', 'President Number']:\n",
    "            double_entry = self.fast_facts['Grover Cleveland'][entry_name]\n",
    "            second_entry_index = 2 if entry_name == 'President Number' else 3\n",
    "\n",
    "            entry_1 = double_entry.split('\\n')[1]\n",
    "            entry_2 = double_entry.split('\\n')[second_entry_index]\n",
    "\n",
    "            self.fast_facts['Grover Cleveland'][entry_name] = entry_1\n",
    "            self.fast_facts['Grover Cleveland 2'][entry_name] = entry_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PotusScraper:\n",
    "    '''\n",
    "    Class for scraping the POTUS webpage to get data about US presidents and elections. \n",
    "\n",
    "    Particularly, it serves for scraping data on presidential salaries (self.salaries), \n",
    "    and election results (self.election_results).\n",
    "    '''\n",
    "\n",
    "    def __init__(self):\n",
    "        ''' Initializes the scraping class. '''\n",
    "        self.origin: str = 'https://www.potus.com/'\n",
    "\n",
    "        self.subdirectories: Optional[Dict[str, Any]] = None\n",
    "\n",
    "        self.salaries: Optional[Dict[str, Any]] = None\n",
    "        self.election_results: Optional[Dict[str, Any]] = None\n",
    "\n",
    "\n",
    "    def get_subdirectories(self) -> None:\n",
    "        '''\n",
    "        Creates a dictionary with the presidents' names as keys and their respective \n",
    "        subdirectories as values, then saves the dictionary to self.subdirectories.\n",
    "        '''\n",
    "        # parse the given origin (web address) utilizing BeautifulSoup \n",
    "        soup = get_soup(self.origin) \n",
    "        # navigate through the soup to get to the part that contains the links \n",
    "        # (subdirectories) to individual pages of US presidents\n",
    "        a_blocks = soup.find_all('a', {'target':'_self'})\n",
    "\n",
    "        subdirectories = {}\n",
    "        for a_block in tqdm(a_blocks): \n",
    "            # each a_block represents one president, extract the subdirectory and save it \n",
    "            # under the president's name\n",
    "            president_and_name_and_years = a_block.find('img')['alt']\n",
    "            president_and_name = president_and_name_and_years.split(',')[0]\n",
    "            name = president_and_name.replace('President ', '')\n",
    "\n",
    "            subdirectories[name] = a_block['href']\n",
    "\n",
    "        # popping the first element which contains the 'Facts About the Presidents' section\n",
    "        subdirectories.pop('Facts About the Presidents')\n",
    "\n",
    "        self.subdirectories = subdirectories\n",
    "\n",
    "\n",
    "    def get_salaries(self) -> None:\n",
    "        '''\n",
    "        Iterates over the subdirectories to get on the individual page of the respective \n",
    "        president and save the relevant salary into self.salaries. \n",
    "        '''\n",
    "        salaries = {}\n",
    "        for president, subdirectory in tqdm(self.subdirectories.items()):\n",
    "            # parse the given path (web address) utilizing BeautifulSoup\n",
    "            soup = get_soup(self.origin + subdirectory)\n",
    "            # navigate through the soup to get to the part relevant for the salary\n",
    "            try:\n",
    "                presidential_salary_title = soup.find(string = \"Presidential Salary:\")\n",
    "                presidential_salary = presidential_salary_title.find_parent('p').text\n",
    "            # Benjamin Harrison has space in the string, hence the exception\n",
    "            except AttributeError:\n",
    "                presidential_salary_title = soup.find(string = \"Presidential Salary: \")\n",
    "                presidential_salary = presidential_salary_title.find_parent('p').text\n",
    "\n",
    "            # save the salary into the dict with salaries\n",
    "            salaries[president] = presidential_salary\n",
    "\n",
    "        self.salaries = salaries\n",
    "\n",
    "\n",
    "    def get_election_results(self) -> None:\n",
    "        '''\n",
    "        Iterates over the subdirectories to get on the individual page of the respective \n",
    "        president and save the relevant salary into self.salaries. \n",
    "        '''  \n",
    "        election_results = {}\n",
    "        for president, subdirectory in tqdm(self.subdirectories.items()):\n",
    "            # parse the given path (web address) utilizing BeautifulSoup\n",
    "            soup = get_soup(self.origin + subdirectory)\n",
    "\n",
    "            # navigate through the soup to get to the part relevant for election results\n",
    "            presidential_elections_title = soup.find(string = \"Presidential Election Results:\")\n",
    "            presidential_elections = presidential_elections_title.find_parent('div')\n",
    "            presidential_elections_tables = presidential_elections.find_all('table')\n",
    "\n",
    "            # extract election results for individual years\n",
    "            for table in presidential_elections_tables:\n",
    "                # extract the year\n",
    "                year = table.find('tr', {'class','row-2'}).find('a').text\n",
    "                election_results[year] = {}\n",
    "\n",
    "                # extract the respective electee results\n",
    "                electee_results = table.find_all('tr')\n",
    "\n",
    "                for electee in electee_results:\n",
    "                    try:\n",
    "                        # extract the name of the candidate\n",
    "                        electee_name = electee.find('td', {'class':'column-2'}).a.text\n",
    "                        election_results[year][electee_name] = {}\n",
    "\n",
    "                        # data contained in tables of early election results do not include the\n",
    "                        # 'popular votes' column, therefore we include this condition\n",
    "                        if len(presidential_elections.find('tr').find_all('th')) == 3: \n",
    "                            # number of popular votes candidate gained\n",
    "                            popular_votes = None\n",
    "                            # number of electoral votes candidate gained\n",
    "                            electoral_votes = electee.find('td', {'class':'column-3'}).text\n",
    "                        else:\n",
    "                            popular_votes = electee.find('td', {'class':'column-3'}).text   \n",
    "                            electoral_votes = electee.find('td', {'class':'column-4'}).text   \n",
    "\n",
    "                        # save election results into the dict with election results\n",
    "                        election_results[year][electee_name]['Popular Votes'] = popular_votes\n",
    "                        election_results[year][electee_name]['Electoral Votes'] = electoral_votes \n",
    "                    # first row is the header, hence the exception\n",
    "                    except AttributeError:\n",
    "                        pass\n",
    "\n",
    "        self.election_results = election_results\n",
    "\n",
    "\n",
    "    def duplicate_Grover_Cleveland_salary(self) -> None:\n",
    "        '''\n",
    "        Duplicates Grover Cleveland's salary, the second entry being recorded for Grover Cleveland's\n",
    "        second term in office. The second entry was not recorded because the two terms were not \n",
    "        consecutive.\n",
    "        '''\n",
    "        # assert that 'Grover Cleveland 2' entry doesn't exist already\n",
    "        assert 'Grover Cleveland 2' not in self.salaries.keys()\n",
    "\n",
    "        # create an entry for Cleveland's second term\n",
    "        self.salaries['Grover Cleveland 2'] = self.salaries['Grover Cleveland']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialiaze the scraping class for Miller Center\n",
    "miller_scrape = MillerScraper()\n",
    "\n",
    "# get the page subdirectories for each president\n",
    "miller_scrape.get_subdirectories()\n",
    "\n",
    "# get data on facts and characteristics (fast_facts), brief descriptions (descriptions), \n",
    "# famous quotes (famous_quotes), and counts of notable events happened during the \n",
    "# president's office (key_events_counts)\n",
    "miller_scrape.get_fast_facts()\n",
    "miller_scrape.get_descriptions()\n",
    "miller_scrape.get_famous_quotes()\n",
    "miller_scrape.get_key_events_counts()\n",
    "\n",
    "# correct Grover Cleveland's data which are flawed due to him serving two non-consecutive\n",
    "# terms\n",
    "miller_scrape.correct_Grover_Cleveland_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialiaze the scraping class for POTUS\n",
    "potus_scrape = PotusScraper()\n",
    "\n",
    "# get the page subdirectories for each president\n",
    "potus_scrape.get_subdirectories()\n",
    "\n",
    "# POTUS uses different formats for names of presidents and also has got some names wrong,\n",
    "# correct the names in subdirectories now so that salaries and election results are saved\n",
    "# under correct names\n",
    "potus_scrape.correct_subdirectories(miller_scrape)\n",
    "\n",
    "# get data on presidential salaries (salaries), and election results (election_results)\n",
    "potus_scrape.get_salaries()\n",
    "potus_scrape.get_election_results()\n",
    "\n",
    "# duplicate Grover Cleveland's salary for his second term, the salary was not recorded twice\n",
    "# because his two terms were not consecutive\n",
    "potus_scrape.duplicate_Grover_Cleveland_salary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: manipulate the data here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_presidents_data_df(\n",
    "    *,\n",
    "    miller_scrape: MillerScraper, \n",
    "    potus_scrape: PotusScraper\n",
    ") -> pd.DataFrame:\n",
    "    ''' \n",
    "    Merges available data about US presidents from miller_scrape and potus_scrape into one \n",
    "    DataFrame.\n",
    "\n",
    "    Forces keyword arguments to avoid messing-up the order (Miller, POTUS).\n",
    "    '''\n",
    "    all_presidents_data = {}\n",
    "    for president in miller_scrape.fast_facts.keys():\n",
    "        all_presidents_data[president] = {\n",
    "            **miller_scrape.fast_facts[president],\n",
    "            'Description': miller_scrape.descriptions[president],\n",
    "            'Famous Quote': miller_scrape.famous_quotes[president],\n",
    "            'Key Events Count': miller_scrape.key_events_counts[president],\n",
    "            'Salary': potus_scrape.salaries[president]\n",
    "        }\n",
    "\n",
    "    return pd.DataFrame(all_presidents_data).T\n",
    "\n",
    "\n",
    "def get_election_results_df(potus_scrape: PotusScraper) -> pd.DataFrame:\n",
    "    ''' \n",
    "    Merges available data on election results from potus_scrape into one DataFrame.\n",
    "\n",
    "    The resulting DataFrame has multiindex columns ('Electoral Votes' and 'Popular Votes'\n",
    "    for each 'Year').\n",
    "    '''\n",
    "    years = []\n",
    "    election_results = []\n",
    "    for year, year_results in potus_scrape.election_results.items():\n",
    "        years.append(year)\n",
    "        election_results.append(pd.DataFrame(year_results))\n",
    "\n",
    "    return pd.concat(election_results, keys = years, sort = True).T\n",
    "\n",
    "\n",
    "def clean_presidents_data(presidents_data: pd.DataFrame) -> pd.DataFrame:\n",
    "    ''' \n",
    "    Replaces problematic strings in the given DataFrame with data about presidents. \n",
    "    '''\n",
    "    PROBLEMATIC_STRINGS = {'\\n', '\\t', '\\r', '\\xa0'}\n",
    "\n",
    "    for problematic_string in PROBLEMATIC_STRINGS:\n",
    "        presidents_data = presidents_data.applymap(\n",
    "            lambda x: x.replace(problematic_string, '') if isinstance(x, str) else x\n",
    "        )\n",
    "\n",
    "    return presidents_data\n",
    "\n",
    "\n",
    "def convert_presidents_data(presidents_data: pd.DataFrame) -> pd.DataFrame:\n",
    "    ''' \n",
    "    Converts values in the given DataFrame with data about presidents to appropriate types.\n",
    "    '''\n",
    "    STR_VARIABLES = {\n",
    "        'Birth Place', \n",
    "        'Burial Place', \n",
    "        'Career', \n",
    "        'Children',\n",
    "        'Description', \n",
    "        'Education', \n",
    "        'Famous Quote',\n",
    "        'Full Name',\n",
    "        'Marriage',\n",
    "        'Nickname',\n",
    "        'Political Party', \n",
    "        'Religion'\n",
    "    }\n",
    "    INT_VARIABLES = {\n",
    "        'Key Events Count',\n",
    "        'President Number',\n",
    "        'Salary'\n",
    "    }\n",
    "    TIMESTAMP_VARIABLES = {\n",
    "        'Birth Date',\n",
    "        'Date Ended', \n",
    "        'Death Date',\n",
    "        'Inauguration Date'\n",
    "    }\n",
    "\n",
    "\n",
    "    def _extract_salary(string: str) -> int:\n",
    "        ''' Extracts salary from string. '''\n",
    "        if not isinstance(string, str):\n",
    "            return string\n",
    "\n",
    "        base = int(string.split('$')[1].split(',')[0]) * 1_000\n",
    "\n",
    "        expense_account = 0\n",
    "        if 'expense' in string:\n",
    "            if len(string.split('$')) > 3:\n",
    "                expense_account = int(string.split('$')[3].split(',')[0]) * 1_000\n",
    "            else:\n",
    "                expense_account = int(string.split('$')[2].split(',')[0]) * 1_000\n",
    "\n",
    "        return base + expense_account\n",
    "\n",
    "\n",
    "    # convert data in respective columns\n",
    "    for str_variable in STR_VARIABLES:\n",
    "        presidents_data[str_variable] = presidents_data[str_variable].map(str)\n",
    "        presidents_data[str_variable] = presidents_data[str_variable].map(\n",
    "            lambda x: None if x in {'None', 'nan'} else x\n",
    "        )\n",
    "    for int_variable in INT_VARIABLES:\n",
    "        if int_variable == 'Salary':\n",
    "            presidents_data[int_variable] = presidents_data[int_variable].map(_extract_salary)\n",
    "        else:\n",
    "            presidents_data[int_variable] = presidents_data[int_variable].map(int)\n",
    "    for timestamp_variable in TIMESTAMP_VARIABLES:\n",
    "        presidents_data[timestamp_variable] = presidents_data[timestamp_variable].map(pd.Timestamp)\n",
    "\n",
    "    return presidents_data\n",
    "\n",
    "\n",
    "def convert_elections_data(elections_data: pd.DataFrame) -> pd.DataFrame:\n",
    "    ''' \n",
    "    Converts values in the given DataFrame with data about elections to appropriate types.\n",
    "    '''\n",
    "    def _extract_votes(string: str) -> Optional[int]:\n",
    "        ''' Extracts number of votes from string. '''\n",
    "        if not isinstance(string, str):\n",
    "            return string\n",
    "\n",
    "        try:\n",
    "            return int(string)\n",
    "        # votes higher than 999 have the 'millions,thousands,units' format and some votes are \n",
    "        # missing, hence the exception\n",
    "        except ValueError:\n",
    "            if string in {'None', 'nan', ''}:\n",
    "                return np.nan\n",
    "\n",
    "            # convert one Popular Votes entry from 2012 containing '.' instead of ','\n",
    "            if '.' in string and ',' in string:\n",
    "                units = int(string.split('.')[-1])\n",
    "                thousands = int(string.split('.')[-2].split(',')[-1])\n",
    "                millions = int(string.split(',')[-2])\n",
    "\n",
    "                return millions * 1_000_000 + thousands * 1_000 + units\n",
    "\n",
    "            units = int(string.split(',')[-1])\n",
    "            thousands = int(string.split(',')[-2])\n",
    "            millions = 0\n",
    "\n",
    "            if len(string.split(',')) == 3:\n",
    "                millions = int(string.split(',')[-3])\n",
    "\n",
    "            return millions * 1_000_000 + thousands * 1_000 + units\n",
    "           \n",
    "\n",
    "    # convert data in every cell\n",
    "    elections_data = elections_data.applymap(str)\n",
    "    elections_data = elections_data.applymap(_extract_votes)\n",
    "\n",
    "    return elections_data\n",
    "\n",
    "\n",
    "def order_presidents_data(presidents_data: pd.DataFrame) -> pd.DataFrame:\n",
    "    ''' Orders data about presidents by 'Inauguration Date'. '''\n",
    "    return presidents_data.sort_values('Inauguration Date')\n",
    "\n",
    "\n",
    "def compute_presidents_features(presidents_data: pd.DataFrame) -> pd.DataFrame:\n",
    "    ''' \n",
    "    Computes features from available data about presidents and adds them as new columns to the\n",
    "    DataFrame. \n",
    "    '''\n",
    "    # compute the features\n",
    "    presidents_data['Years at Inauguration'] = (\n",
    "        presidents_data['Inauguration Date'] - presidents_data['Birth Date']\n",
    "    ).map(lambda x: x.days / 365)\n",
    "\n",
    "    return presidents_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the available data for presidents and elections\n",
    "all_presidents_data_df = get_all_presidents_data_df(\n",
    "    miller_scrape = miller_scrape, \n",
    "    potus_scrape = potus_scrape\n",
    ")\n",
    "election_results_df = get_election_results_df(potus_scrape)\n",
    "\n",
    "# clean presidents data\n",
    "all_presidents_data_df = clean_presidents_data(all_presidents_data_df)\n",
    "# election_results_df do not need to be cleaned, they do not contain problematic values\n",
    "\n",
    "# convert values to appropriate types\n",
    "all_presidents_data_df = convert_presidents_data(all_presidents_data_df)\n",
    "election_results_df = convert_elections_data(election_results_df)\n",
    "\n",
    "# order presidents data\n",
    "all_presidents_data_df = order_presidents_data(all_presidents_data_df)\n",
    "\n",
    "# compute new features\n",
    "all_presidents_data_df = compute_presidents_features(all_presidents_data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Years at Inauguration'] = (data['Inauguration Date'] - data['Birth Date']).map(lambda x: x.days / 365)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: make the visualizations here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_years_at_inauguration():\n",
    "    trace = plotly.graph_objs.Scatter(\n",
    "        x = data['Inauguration Date'],\n",
    "        y = data['Years at Inauguration'],\n",
    "        mode = 'lines+markers',\n",
    "        name = 'chart_a',\n",
    "        marker = {'size': 7},\n",
    "    )\n",
    "\n",
    "    layout = plotly.graph_objs.Layout(\n",
    "        title = 'Years at Inauguration',\n",
    "        yaxis = {'title': 'Years at Inauguration'},\n",
    "        xaxis = {'title': 'Inauguration Date'},\n",
    "    )\n",
    "\n",
    "    plotly.offline.iplot(plotly.graph_objs.Figure(\n",
    "        data = [trace],\n",
    "        layout = layout\n",
    "    ))\n",
    "\n",
    "plot_years_at_inauguration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_number_of_major_events():\n",
    "    trace = plotly.graph_objs.Scatter(\n",
    "        x = data['Inauguration Date'],\n",
    "        y = data['Number of major events'],\n",
    "        mode = 'lines+markers',\n",
    "        name = 'chart_a',\n",
    "        marker = {'size': 7},\n",
    "    )\n",
    "\n",
    "    layout = plotly.graph_objs.Layout(\n",
    "        title = 'Number of major events',\n",
    "        yaxis = {'title': 'Number of major events'},\n",
    "        xaxis = {'title': 'Inauguration Date'},\n",
    "    )\n",
    "\n",
    "    plotly.offline.iplot(plotly.graph_objs.Figure(\n",
    "        data = [trace],\n",
    "        layout = layout\n",
    "    ))\n",
    "\n",
    "plot_number_of_major_events()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: conclude here?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
