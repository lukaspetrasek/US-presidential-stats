{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# US presidential statistics\n",
    "## Simon Repko, Lukas Petrasek\n",
    "### IES FSS CU\n",
    "### 31.5.2019\n",
    "\n",
    "This notebook serves as a demonstration of a school project whose goal is to achieve the following:\n",
    "* scrape web pages to get historical data on US presidents\n",
    "* manipulate the data into a form suitable for being visualized\n",
    "* make vizualizations based the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, Optional, Type\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import plotly\n",
    "import requests\n",
    "\n",
    "\n",
    "\n",
    "plotly.offline.init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: initialize the scraping class and apply the methods necessary to get the data here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_soup(url: str) -> BeautifulSoup:\n",
    "    ''' Returns soup for the given url. '''\n",
    "    return BeautifulSoup(requests.get(url).text, 'html.parser')\n",
    "\n",
    "\n",
    "\n",
    "class MillerScraper:\n",
    "    '''\n",
    "    Class for scraping the Miller Center webpage to get data about US presidents. \n",
    "\n",
    "    Particularly, it serves for scraping data on facts and characteristics (self.fast_facts), \n",
    "    brief description (self.descriptions), famous quotes (self.famous_quotes), and count of \n",
    "    notable events happened during the president's office (self.key_events_counts).\n",
    "    '''\n",
    "\n",
    "    def __init__(self):\n",
    "        ''' Initializes the scraping class. '''\n",
    "        self.origin: str = 'https://millercenter.org/'\n",
    "\n",
    "        self.subdirectories: Optional[Dict[str, Any]] = None\n",
    "\n",
    "        self.fast_facts: Optional[Dict[str, Any]] = None\n",
    "        self.descriptions: Optional[Dict[str, Any]] = None\n",
    "        self.famous_quotes: Optional[Dict[str, Any]] = None\n",
    "        self.key_events_counts: Optional[Dict[str, Any]] = None\n",
    "\n",
    "        self.all_presidents_data: Optional[pd.DataFrame()] = None\n",
    "\n",
    "\n",
    "    def get_subdirectories(self) -> None:\n",
    "        '''\n",
    "        Creates a dictionary with the presidents' names as keys and their respective \n",
    "        subdirectories as values, then saves the dictionary to self.subdirectories.\n",
    "        '''\n",
    "        # parse the given origin (web address) utilizing BeautifulSoup \n",
    "        soup = get_soup(self.origin) \n",
    "        # enter the main navigation panel and find submenu that contains the links \n",
    "        # (subdirectories) to individual pages of US presidents\n",
    "        navigation_menu = soup.find('nav', {'aria-labelledby':'block-mainnavigation-3-menu'})\n",
    "        submenu = navigation_menu.find_all('ul', {'class':'submenu'})[1]\n",
    "        a_blocks = submenu.find_all('a')\n",
    "\n",
    "        subdirectories = {}\n",
    "        for a_block in tqdm(a_blocks): \n",
    "            # each a_block represents one president, extract the subdirectory and save \n",
    "            # it under the president's name\n",
    "            subdirectories[a_block.text] = a_block['href']\n",
    "\n",
    "        self.subdirectories = subdirectories\n",
    "\n",
    "\n",
    "    def get_fast_facts(self) -> None:\n",
    "        '''\n",
    "        Iterates over the subdirectories to get on the individual page of the respective \n",
    "        president and save the relevant fast facts into self.fast_facts. \n",
    "        '''\n",
    "        fast_facts = {}\n",
    "        for president, subdirectory in tqdm(self.subdirectories.items()):\n",
    "            # parse the given path (web address) utilizing BeautifulSoup\n",
    "            soup = get_soup(self.origin + subdirectory)\n",
    "            # navigate through the soup to get to the part relevant for fast facts\n",
    "            president_page = soup.find('div', {'class':'president-main-wrapper'})\n",
    "            fast_facts_dashboard = president_page.find('div', {'class':'fast-facts-wrapper'})   \n",
    "\n",
    "            # avoiding redundant elements (containing '\\n')\n",
    "            relevant_fast_facts = [x for x in fast_facts_dashboard.children if x != '\\n']\n",
    "            # popping the first element which contains just the 'Fast Facts' heading\n",
    "            relevant_fast_facts.pop(0)\n",
    "\n",
    "            fast_facts[president] = {}\n",
    "            for fast_fact in relevant_fast_facts:\n",
    "                # save the fast fact under its label into the dict of the given president\n",
    "                fast_facts[president][fast_fact.label.text] = fast_fact.div.text\n",
    "\n",
    "        self.fast_facts = fast_facts\n",
    "\n",
    "\n",
    "    def get_descriptions(self) -> None:\n",
    "        '''\n",
    "        Iterates over the subdirectories to get on the individual page of the respective \n",
    "        president and save the relevant description into self.descriptions. \n",
    "        '''\n",
    "        descriptions = {}\n",
    "        for president, subdirectory in tqdm(self.subdirectories.items()):\n",
    "            # parse the given path (web address) utilizing BeautifulSoup\n",
    "            soup = get_soup(self.origin + subdirectory)\n",
    "            # navigate through the soup to get to the part relevant for the description\n",
    "            description_paragraph = soup.find('div', {'class':'copy-wrapper'})\n",
    "\n",
    "            # save the description into the dict with descriptions\n",
    "            descriptions[president] = description_paragraph.p.text\n",
    "\n",
    "        self.descriptions = descriptions\n",
    "\n",
    "\n",
    "    def get_famous_quotes(self) -> None:\n",
    "        '''\n",
    "        Iterates over the subdirectories to get on the individual page of the respective \n",
    "        president and save the relevant famous quote into self.famous_quotes. \n",
    "        '''\n",
    "        famous_quotes = {}\n",
    "        for president, subdirectory in tqdm(self.subdirectories.items()):\n",
    "            # parse the given path (web address) utilizing BeautifulSoup\n",
    "            soup = get_soup(self.origin + subdirectory)\n",
    "            # navigate through the soup to get to the part relevant for the famous quote\n",
    "            famous_quote_paragraph = soup.find('blockquote', {'class':'president-quote'})\n",
    "\n",
    "            # save the famous quote into the dict with famous quotes\n",
    "            famous_quotes[president] = famous_quote_paragraph.contents[0]\n",
    "\n",
    "        self.famous_quotes = famous_quotes\n",
    "\n",
    "\n",
    "    def get_key_events_counts(self) -> None:\n",
    "        '''\n",
    "        Iterates over the subdirectories to get on the individual page of the respective \n",
    "        president and save the relevant key events count into self.key_events_counts. \n",
    "        '''\n",
    "        key_events_counts = {}\n",
    "        for president, subdirectory in tqdm(self.subdirectories.items()):\n",
    "            # parse the given path (web address) utilizing BeautifulSoup\n",
    "            soup = get_soup(self.origin + subdirectory + '/key-events')\n",
    "            # navigate through the soup to get to the part relevant for key events\n",
    "            key_events_overview = soup.find('div', {'class':'article-wysiwyg-body'})\n",
    "\n",
    "            try:\n",
    "                # count of all events ('titles' highlighted in bold)\n",
    "                key_events_count_bold = len(key_events_overview.find_all('strong'))\n",
    "                # count of all events ('titles' no longer highlighted in bold)\n",
    "                key_events_count_not_bold = len(key_events_overview.find_all('b'))\n",
    "                # sum both counts\n",
    "                key_events_count = key_events_count_bold + key_events_count_not_bold\n",
    "            # D. Trump page has no information about major events, hence the exception \n",
    "            except AttributeError:\n",
    "                key_events_count = 0\n",
    "                pass\n",
    "\n",
    "            # save the key events count into the dict with key events counts\n",
    "            key_events_counts[president] = key_events_count\n",
    "\n",
    "        self.key_events_counts = key_events_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PotusScraper:\n",
    "    '''\n",
    "    Class for scraping the POTUS webpage to get data about US presidents and elections. \n",
    "\n",
    "    Particularly, it serves for scraping data on presidential salaries (self.salaries), \n",
    "    and election results (self.election_results).\n",
    "    '''\n",
    "\n",
    "    def __init__(self):\n",
    "        ''' Initializes the scraping class. '''\n",
    "        self.origin: str = 'https://www.potus.com/'\n",
    "\n",
    "        self.subdirectories: Optional[Dict[str, Any]] = None\n",
    "\n",
    "        self.salaries: Optional[Dict[str, Any]] = None\n",
    "        self.election_results: Optional[Dict[str, Any]] = None\n",
    "\n",
    "\n",
    "    def get_subdirectories(self) -> None:\n",
    "        '''\n",
    "        Creates a dictionary with the presidents' names as keys and their respective \n",
    "        subdirectories as values, then saves the dictionary to self.subdirectories.\n",
    "        '''\n",
    "        # parse the given origin (web address) utilizing BeautifulSoup \n",
    "        soup = get_soup(self.origin) \n",
    "        # navigate through the soup to get to the part that contains the links \n",
    "        # (subdirectories) to individual pages of US presidents\n",
    "        a_blocks = soup.find_all('a', {'target':'_self'})\n",
    "\n",
    "        subdirectories = {}\n",
    "        for a_block in tqdm(a_blocks): \n",
    "            # each a_block represents one president, extract the subdirectory and save \n",
    "            # it under the president's name\n",
    "            president_and_name_and_years = a_block.find('img')['alt']\n",
    "            president_and_name = president_and_name_and_years.split(',')[0]\n",
    "            name = president_and_name.replace('President ', '')\n",
    "\n",
    "            subdirectories[name] = a_block['href']\n",
    "\n",
    "        # popping the first element which contains the 'Facts About the Presidents' section\n",
    "        subdirectories.pop('Facts About the Presidents')\n",
    "\n",
    "        self.subdirectories = subdirectories\n",
    "\n",
    "\n",
    "def getjoinSoupPotus(link):\n",
    "    html = \"\".join(['https://www.potus.com/',link]) \n",
    "    response = requests.get(html)\n",
    "    return BeautifulSoup(response.text,'html.parser') \n",
    "\n",
    "def getDataPotus(html,typeofreturn):\n",
    "    soup = getSoup(html) # Parsing of given html utilizing BeautifulSoup \n",
    "\n",
    "    pres_list = {}\n",
    "    for pres in soup.find_all('a',{'target':'_self'}):\n",
    "        pres_list[pres.find('img')['alt']] = pres['href'] # extract and save names (key) and link 'endings' (value)\n",
    "\n",
    "    pres_list.pop('Facts About the Presidents') # removing first row as it is a title\n",
    "    # dictionary_1 -> includes names and links of each president\n",
    "    \n",
    "    \n",
    "    #1 main loop: iterates links from dictionary_1 and gathers data about US presidential election results and \n",
    "    #             presidential salary of each president - therefore it returns two outputs, conditional on the input value \n",
    "    elec_results = {} #creation of empty final dict of election results\n",
    "    salary = {}       #creation of empty dict for salaries\n",
    "    \n",
    "    for name,href in pres_list.items(): \n",
    "        soup = getjoinSoupPotus(href)\n",
    "\n",
    "        #2 extraction of salary each president received \n",
    "        if typeofreturn == 'salary':\n",
    "            #Benjamin Harrison has space in the string - error msg - therefore workaround is here\n",
    "            try:\n",
    "                salary[name] = soup.find(string=\"Presidential Salary:\").find_parent('p').text\n",
    "            except AttributeError:\n",
    "                salary[name] = soup.find(string=\"Presidential Salary: \").find_parent('p').text\n",
    "\n",
    "        #3 extraction of html tables of Presidential election results       \n",
    "        elif typeofreturn == 'election':\n",
    "            \n",
    "            elections = soup.find(string=\"Presidential Election Results:\").find_parent('div')\n",
    "\n",
    "            #3.1 extraction of results for individual years\n",
    "            for tbl in elections.find_all('table'): #selecting tables for individual years\n",
    "\n",
    "                year = tbl.find('tr',{'class','row-2'}).find('a').text #year extraction\n",
    "                electee_list = {}\n",
    "\n",
    "                #3.2 selecting row with individual candidate\n",
    "                for electee in tbl.find_all('tr'): \n",
    "                    try: # first row is header hence we include this exception skip it and to continue\n",
    "\n",
    "                        electee_name = electee.find('td',{'class':'column-2'}).a.text  # name of candidate\n",
    "                        electee_votes = {}\n",
    "\n",
    "                        #3.3 data contained in tables for results of elections further in the past do not include\n",
    "                        #  'popular votes' column, therefore it was necessary to include this condition\n",
    "                        if len(elections.find('tr').find_all('th')) == 3: \n",
    "                            electee_votes['popular_votes']   = \"\" \n",
    "                            # above: number of popular votes candidate gained\n",
    "                            electee_votes['electoral_votes'] = electee.find('td',{'class':'column-3'}).text\n",
    "                            # above: number of electoral votes candidate gained\n",
    "                        else:\n",
    "                            electee_votes['popular_votes']   = electee.find('td',{'class':'column-3'}).text   \n",
    "                            electee_votes['electoral_votes'] = electee.find('td',{'class':'column-4'}).text   \n",
    "\n",
    "                        electee_list[electee_name] = electee_votes \n",
    "                        \n",
    "                    except AttributeError: # first row is header hence we include this exception to continue\n",
    "                        pass\n",
    "                    \n",
    "                elec_results[year] = electee_list # writing in the results for given year    \n",
    "    \n",
    "    if typeofreturn == 'election':\n",
    "        return elec_results \n",
    "        # dictionary_2 -> contains results of elections grouped by years, states names of the candidates and respective votes \n",
    "        #                 gained\n",
    "    elif typeofreturn == 'salary':\n",
    "        return salary \n",
    "        # dictionary_3 -> contains information about salaries of each president (strings as it includes additional information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialiaze the scraping class for Miller Center\n",
    "miller_scrape = MillerScraper()\n",
    "\n",
    "# get the page subdirectories for each president\n",
    "miller_scrape.get_subdirectories()\n",
    "\n",
    "# get data on facts and characteristics (fast_facts), brief descriptions (descriptions), \n",
    "# famous quotes (famous_quotes), and counts of notable events happened during the \n",
    "# president's office (key_events_counts)\n",
    "miller_scrape.get_fast_facts()\n",
    "miller_scrape.get_descriptions()\n",
    "miller_scrape.get_famous_quotes()\n",
    "miller_scrape.get_key_events_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_elections = getDataPotus('https://www.potus.com/','election') # execution time over 1 minute 35 sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_salary = getDataPotus('https://www.potus.com/','salary') # execution time over 1 minute 35 sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: manipulate the data here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grover Cleveland was in office 2 non-consecutive times\n",
    "data_presidents['Grover Cleveland 2'] = {\n",
    "    key: value for key, value in data_presidents['Grover Cleveland'].items() if key not in ['Inauguration Date', 'Date Ended']\n",
    "}\n",
    "\n",
    "def correct_Grover_Cleveland_dates(data: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    inauguration_date_1 = data['Grover Cleveland']['Inauguration Date'].split('\\n')[1]\n",
    "    inauguration_date_2 = data['Grover Cleveland']['Inauguration Date'].split('\\n')[3]\n",
    "    date_ended_1 = data['Grover Cleveland']['Date Ended'].split('\\n')[1]\n",
    "    date_ended_2 = data['Grover Cleveland']['Date Ended'].split('\\n')[3]\n",
    "\n",
    "    data['Grover Cleveland']['Inauguration Date'] = inauguration_date_1\n",
    "    data['Grover Cleveland 2']['Inauguration Date'] = inauguration_date_2\n",
    "    data['Grover Cleveland']['Date Ended'] = date_ended_1\n",
    "    data['Grover Cleveland 2']['Date Ended'] = date_ended_2\n",
    "\n",
    "    return data\n",
    "\n",
    "# run only once\n",
    "data_presidents = correct_Grover_Cleveland_dates(data_presidents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final table with data extracted from https://millercenter.org\n",
    "# pd.DataFrame(data_presidents)\n",
    "\n",
    "data = pd.DataFrame(data_presidents).applymap(lambda x: x.replace('\\n', '') if isinstance(x, str) else x).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_variables = {\n",
    "    'Birth Place', \n",
    "    'Burial Place', \n",
    "    'Career', \n",
    "    'Children',\n",
    "    'Description', \n",
    "    'Education', \n",
    "    'Full Name',\n",
    "    'Marriage',\n",
    "    'Nickname',\n",
    "    'Political Party', \n",
    "    'Quote',\n",
    "    'Religion'\n",
    "}\n",
    "\n",
    "int_variables = {\n",
    "    'Number of major events',\n",
    "    'President Number'\n",
    "}\n",
    "\n",
    "timestamp_variables = {\n",
    "    'Birth Date',\n",
    "    'Date Ended', \n",
    "    'Death Date',\n",
    "    'Inauguration Date'\n",
    "}\n",
    "\n",
    "for str_variable in str_variables:\n",
    "    data[str_variable] = data[str_variable].map(str)\n",
    "\n",
    "for int_variable in int_variables:\n",
    "    data[int_variable] = data[int_variable].map(int)\n",
    "\n",
    "for timestamp_variable in timestamp_variables:\n",
    "    data[timestamp_variable] = data[timestamp_variable].map(pd.Timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Years at Inauguration'] = (data['Inauguration Date'] - data['Birth Date']).map(lambda x: x.days / 365)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: make the visualizations here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_years_at_inauguration():\n",
    "    trace = plotly.graph_objs.Scatter(\n",
    "        x = data['Inauguration Date'],\n",
    "        y = data['Years at Inauguration'],\n",
    "        mode = 'lines+markers',\n",
    "        name = 'chart_a',\n",
    "        marker = {'size': 7},\n",
    "    )\n",
    "\n",
    "    layout = plotly.graph_objs.Layout(\n",
    "        title = 'Years at Inauguration',\n",
    "        yaxis = {'title': 'Years at Inauguration'},\n",
    "        xaxis = {'title': 'Inauguration Date'},\n",
    "    )\n",
    "\n",
    "    plotly.offline.iplot(plotly.graph_objs.Figure(\n",
    "        data = [trace],\n",
    "        layout = layout\n",
    "    ))\n",
    "\n",
    "plot_years_at_inauguration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_number_of_major_events():\n",
    "    trace = plotly.graph_objs.Scatter(\n",
    "        x = data['Inauguration Date'],\n",
    "        y = data['Number of major events'],\n",
    "        mode = 'lines+markers',\n",
    "        name = 'chart_a',\n",
    "        marker = {'size': 7},\n",
    "    )\n",
    "\n",
    "    layout = plotly.graph_objs.Layout(\n",
    "        title = 'Number of major events',\n",
    "        yaxis = {'title': 'Number of major events'},\n",
    "        xaxis = {'title': 'Inauguration Date'},\n",
    "    )\n",
    "\n",
    "    plotly.offline.iplot(plotly.graph_objs.Figure(\n",
    "        data = [trace],\n",
    "        layout = layout\n",
    "    ))\n",
    "\n",
    "plot_number_of_major_events()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: conclude here?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
